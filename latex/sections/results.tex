%================================================================
\section{Results}\label{sec:results}
% Present results and give a critical discussion of my work in 
% the context of other work. Relate the work to previous studies, 
% and make sure the results are reproducible. Include information 
% in figure captions such that they can give the reader enough to 
% understand the main gist of the report.
%================================================================

\subsection{Gradient descent}

We applied the gradient descent algorithm to data simulated from a third-degree polynomial without noise to assess convergence to the true parameters (APPENDIX). All parameters converged after approximately 800 iterations using plain gradient descent (figure \ref{fig:poly-converge}). Adding momentum to the gradient descent sped up convergence to around 4-fold (figure \ref{fig:poly-converge-momentum}), and using any optimizer had similar effects on convergence times (APPENDIX FIGS).

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{examples/tests_even/figs/gradient-descent-polynomial-convergence.pdf}
    \caption{Gradient descent on data generated from a 4th degree polynomial without noise, using polynomial features up to the 4th degree. The stippled line represents 0 difference from the parameters used to generate the data.}
    \label{fig:poly-converge}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{examples/tests_even/figs/gradient-descent-momentum-polynomial-convergence.pdf}
    \caption{Gradient descent on the same data as in Figure \ref{fig:poly-converge}, but using momentum. The algorithm converged considerably faster than gradient descent without momentum.}
    \label{fig:poly-converge-momentum}
\end{figure}

The choice of learning rate affected the performance of the model regardless of optimizer. For data generated from the Franke function \cite{franke1979} with added noise, the different optimizers had different optimal learning rates, but in general a learning rate between $10^{-3}$ and $1$ gave the best results (figure \ref{fig:franke-learningrate}). For this particular data GD outperformed SGD (APPENDIX). Overall, ADAM consistently gave the lowest mean squared error on the test data over the largest range of learning rates. Because of this, we chose to emphasize ADAM over the other optimizers when analyzing the terrain data.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{examples/tests_even/figs/Franke-learningrates-optimizers.pdf}
    \caption{How learning rates affect the mean squared error in gradient descent with momentum or different optimizers. The data was generated from the Franke function, with added noise. For the momentum algorithm the gradient descent was not completed for learning rates above $10^{-2}$ due to exploding gradients.}
    \label{fig:franke-learningrate}
\end{figure}

For gradient descent on the terrain data ...

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{examples/tests_even/figs/gradient-descent-terrain-map.pdf}
    \caption{Prediction of the terrain data from project 1 using gradient descent with a ridge gradient.}
    \label{fig:enter-label}
\end{figure}



- describe implementation\\
- Show all the different optimizers etc\\
- compare to regression from last project\\
- Based on this, we chose to keep this and this optimizer\\
- include figure of terrain, compare to ridge from last project and data\\



\subsection{Regression analysis with FFNN}

- compare to project 1\\
- discuss results compared to OLS/Ridge\\
- Optimal learning rates and parameters, lambda-learningrate grid-search\\
- discuss activation functions\\
- discuss initialization of learning rates\\


\subsection{Breast cancer data}
We tested a set of different optimizers and learning rates on our breast cancer (BC) data, and found the best optimizer to be Adam, as is the golden standard for many NN problems today. The optimal learning rate with Adam was 0.1. To perform the classification we chose two layers of size 100 and 2, where our final layer output was a $n x 2$ vector with probabilities. For information about this selection process, see our appendix. "cite something?"

After splitting and normalizing the input data, we trained our neural net against Sklearns MLPClassifier with the same amount of layers. We chose the ReLU6 activation function for the first 100-node layer, and the final layer was sigmoidal. For argumentation, see Subsection \ref{sssec:activation_functions}.

To prevent potential unwelcome batch effects that can occur from random splitting of test and train data, we did a 10-fold cross validation inside the NN-data and chose the back-propagation path with the best accuracy. The result was suprisingly that our neural network performed better than Sklearn to a very small degree, as presented below: 




- A couple of sentences on choosing optimizer
- Classification and accuracy of binary classification\\
- Discuss the results and analyze the different parameters used (learning rate, lambda) and the effect of different activation functions\\
- Compare with similar code from Scikit-Learn etc.\\
- Compare with logistic regression\\

