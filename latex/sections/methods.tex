%==============================================================
\section{Methods}\label{sec:methods}
% Describe the methods and algorithms used, include any formulas. 
% Explain how everything is implemented, and possibly mention the structure of the algorithm. 
% Add demonstrations such as tests, selected runs and validations. 
%==============================================================
%------------ Background? -------------------------------------
\subsection{Regression vs. Classification}\label{ssec:regression_classification}
In regression problems the aim is to find a functional relationship between a dependent variable, and one or more independent variables. For linear regression the outcome is a linear function which approximates this relationship. For terrain data, the input are coordinates and the output is the height.

In classification, however, the aim is to separate the outputs into classes, and the model assigns a class for the input. We focused on two classification methods for predicting breast cancer, specifically logistic regression and neural networks.


%------------ Logistic Regression -----------------------------
\subsection{Logistic Regression}\label{ssec:logreg}
Logistic regression is a method of classification, which estimates the probability of being in a certain class. In contrast to linear regression methods, where the outcome is an approximation of a continuous function, the outcome of logistic regression is a classifier which gives decision boundaries between classes. This method is often used as a baseline model, particularly in problems in the nature of binary classification, which is what we will focus on.

The sigmoid function in Equation \eqref{eq:sigmoid}, is used to assign a class to the input data, by determining the likelihood of that event.
\begin{equation}\label{eq:sigmoid}
    p(z) = \frac{1}{1 + \exp{-z}} , 
\end{equation}
where $z$ is the model's predicted outcome. We define the cost function as the log-likelihood in Equation \eqref{eq:log_likelihood}, which is derived from the Maximum Likelihood Principle \cite[p. 31]{hastie:2009:elements}.
\begin{equation}\label{eq:log_likelihood}
\begin{split}
    \mathcal{C}(\mathbf{\beta}) = \sum_{i=1}^{n} & (y_{i} \log p(y_{i} = 1 | x_{i}, \beta) \\
    & + (1 - y_{i}) \log (1 - p(y_{i} = 1 | x_{i}, \beta x_{i})))
\end{split}
\end{equation}
Re-writing the logarithms, and maximizing with respect to $\beta$ result in the cross entropy cost function in Equation \eqref{eq:cross_entropy}.
\begin{equation}\label{eq:cross_entropy}
\begin{split}
    \mathcal{C}(\mathbf{\beta}) = - \sum_{i=1}^{n} & (y_{i} (\beta_{0} + \beta_{1} x_{i}) \\
    &- \log (1 + \exp{\beta_{0} + \beta_{1} x_{i}})))
\end{split}
\end{equation}
Here we can also add a term of regulatization, such as the $L_{2}$ regularization 
\begin{equation}
    L_{2} = \lambda || \beta ||_{2}^{2} ,
\end{equation}
where $\lambda$ is the penalty parameter.

The cost function is a convex function, and by minimizing it we find the derivatives
\begin{equation}\label{eq:d_cross_entropy}
    \frac{\partial \mathcal{C} \beta}{\partial \beta} = - \mathbf{X}^{T} (\mathbf{y} - \mathbf{p})
\end{equation}
and 
\begin{equation}\label{eq:dd_cross_entropy}
    \frac{\partial^{2} \mathcal{C} \beta}{\partial \beta \partial \beta^{T}} = \mathbf{X}^{T} \mathbf{W} \mathbf{X} .
\end{equation}
The matrix $\mathbf{X}$ contains the input values, $\mathbf{y}$ the target classes, $\mathbf{p}$ the probabilities of an outcome class, and $\mathbf{W}$ is the diagonal matrix with the product of the probabilities. 

To find the minima of our function, we used both the gradient descent and stochastic gradient descent methods.

%------------ Gradient Descent --------------------------------
\subsection{Gradient Descent}\label{ssec:gradient_descent}

%% work-in-progress her, må samle tankene for å forklare dette på en god måte!
For both regression and classification, we want to optimize a set of parameters $\theta$ given a cost function $C(X, \theta)$, usually by minimizing the cost function. One way of finding the minimum of the cost function given our parameters is by gradient descent (GD) (Algorithm \ref{alg:gd}). In gradient descent you start with a random set of parameters, and change these in small steps towards the optimal values by moving iteratively along a gradient \cite{Goodfellow:2016:deep_learning}. The gradient is found by calculating the first derivatives of the cost function with respect to the parameters, and evaluating these for each iteration. The rate of descent for each parameter is ideally determined using Newton's method, but this requires the often prohibitively expensive operation of calculating the Hessian matrix \cite{battiti1992:newtons_method}. Instead, it is common use a fixed step size $\eta$, known as the learning rate of the model. The algorithm is run either until some convergence criterion is reached (e.g., gradients approaching 0) it reaches the maximum number of iterations.

\begin{algorithm}
\caption{Gradient descent}\label{alg:gd}
\begin{algorithmic}[1]
    \STATE Initialize parameters $\theta = \theta_0$
    \STATE Choose a learning rate $\eta > 0$
    \STATE Set number of iterations K
    \FOR{iteration $=1$ to K}
        \STATE Compute the gradient $\nabla C(\theta)$
        \STATE Update the parameters: $\theta \leftarrow \theta - \eta \nabla C(\theta)$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

While gradient descent effectively minimizes the cost function given the starting parameters, the algorithm can find a local minimum, rather than the lower global minimum. Additionally, gradient descent is computationally expensive for large data sets with many features. It is common to instead use a small random subset of your data each time you compute the gradients, rather than the full data, known as stochastic gradient descent (SGD, also known as mini-batch gradient descent) (Algorithm \ref{alg:sgd}). SGD increases the chance of avoiding local minima, and is more computationally efficient than GD \cite{Goodfellow:2016:deep_learning}.

\begin{algorithm}
\caption{Stochastic gradient descent with mini-batches}\label{alg:sgd}
\begin{algorithmic}[1]
    \STATE Initialize parameters $\theta = \theta_0$
    \STATE Choose a learning rate $\eta > 0$
    \STATE Choose mini-batch size $m$
    \STATE Set number of epochs $K$
    \FOR{epoch $= 1$ to $K$}
        \STATE Shuffle the training data
        \FOR{each mini-batch \( \mathcal{B}_i \) of size $m$}
            \STATE Compute the gradient: $\frac{1}{m}\nabla_{\mathcal{B}_i} C(\theta)$
            \STATE Update the parameters: $\theta \leftarrow \theta - \eta \nabla_{\mathcal{B}_i} C(\theta)$
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Rather than simply using the current gradient and a fixed learning rate $\eta$, it is common to also use information from the previous step when determining the size and direction of the current step. One way to do this is by adding the previous step along the gradient multiplied with a constant $\gamma$ to the current gradient step, as
\begin{equation}
    \mathbf{v}_{t} = \eta \nabla C(\boldsymbol{\theta}) + \gamma \mathbf{v}_{t-1}
\end{equation}
\begin{equation}
    \boldsymbol{\theta}_{t+1}= \boldsymbol{\theta}_t -\mathbf{v}_{t},
\end{equation}
    
where $\theta$ is the parameters, and $\mathbf{v}_t$ and $\mathbf{v}_{t-1}$ is the current and previous change in the gradient, respectively. This method is known as gradient descent with momentum, and can be used with both GD and SGD. Adding momentum changes both the direction and magnitude of the steps based on the previous gradients, and often improves convergence times \cite{Goodfellow:2016:deep_learning}.

As the optimal learning rate for reaching a minimum often changes as you iterate, several algorithms (optimizers) exist to adaptively change the learning rate during gradient descent. Three common optimizers are AdaGrad \cite{duchi2011:adagrad}, RMSProp \cite{hinton2012:rmsprop}, and Adam \cite{kingma2017:Adam}. These methods all scale the learning rate using the accumulated square gradients during the course of training. While AdaGrad simply divides the learning rate by the square root of the summed square gradients, RMSProp implements a decay rate so the more recent gradients have a larger contribution than earier ones. Adam can be seen as an implementation of RMSProp with momentum, but that also rescales the gradients and applies bias-correction before scaling the learning rate \cite{Goodfellow:2016:deep_learning}. RMSProp and Adam tends to outperform AdaGrad, and is frequently used in machine learning \cite{Goodfellow:2016:deep_learning}.




%------------ Feed-Forward Neural Network ---------------------
\subsection{Feed-Forward Neural Network}\label{ssec:ffnn}
In recent years, neural networks have shown promise in solving both regression and classification problems. They and have evolved into several types of networks, with the simplest one called feed-forward neural network (FFNN). In FFNNs, information moves through the layers in one direction, and the network is said to be fully connected if each neuron in a layer is connected to all neurons in the next layer, illustrated in Figure \ref{fig:ffnn}. 

\begin{figure}[h!]
    \centering
    \resizebox{0.9\linewidth}{!}
    {\input{latex/sections/tikz/ffnn}}
    %\includegraphics[width=0.5\linewidth]{}
    \caption{Illustration of a feed-forward neural network with one input layer, three hidden layers, and one output layer, where $n$, $m$ and $k$ indicate the number of neurons in the respective layer.}
    \label{fig:ffnn}
\end{figure}
The architecture of a neural network is often determined by the problem to be solved. According to the universal 
approximation theorem, a FFNN with a minimum of one input layer, one hidden layer with a non-linear activation 
function, and one linear output layer, is sufficient to approximate a continuous function \cite[194]{Goodfellow:2016:deep_learning}. 

The output of a hidden layer can be written as 
\begin{equation}\label{eq:ffnn}
    \mathbf{h} = a \Big( \mathbf{W}^{T} \mathbf{x} + \mathbf{b} \Big) ,
\end{equation}
where $a$ is the activation function, $\mathbf{W}$ is the weight matrix, $\mathbf{x}$ is input, and $\mathbf{b}$ is the bias. 

\subsubsection{Weights and biases}\label{sssec:weights_biases}
The technique used to initialize the weights and biases in the FFNN can be vital for how fast the network learns. 
For a less ideal method, the network may need more iterations to find an optimal solution. 
Whereas a clever initialization may reduce the number of iteration needed, as the weight are some of the 
hyper-parameters that needs tuning. 

For this project, we have chosen to initialize most of our biases as 0 across all layers, as this is common 
practice in the field. Our model does however have the option to initialize the final bias vector as the mean 
of the target's when doing regression modelling, as this is meant to give the backpropagation optimalization 
process a head start when finding the optimal weights and biases during training, as described here: \cite{best_bias}

Initialization of the weights depend on the activation layer present in the layer. When the activation 
function was non-linear and sigmoidal (such as softmax and sigmoidal), weights were initialized after
the Xavier/Gloriot Normal Initialization method \cite{xaviergloriot}. 

After implementation of different linear activation functions (ReLU), we added the option to initialze
weights following the He normal initialization \cite{heman}, as this is supposed to 
lead to faster convergence of the neural network when using ReLU activation. 

\subsubsection{Activation functions}\label{sssec:activation_functions}

Activation functions in neural networks are meant to represent neuronal firing, a process which in simplified terms is either on or off. In early neural networks, the activation function were often sigmoidal, with outputs between 0 and 1, depending on the input. If 0, the neuron would be considered "dead" and not contribute to the final prediction. If 1, the neuron is completely "on" and will pass on the information to the next node. Today however, sigmoidal functions are often replaced with rectified linear units with different outputs. 

Here we separate between sigmoidal nonlinearities and rectified linear hidden units (ReLU). The latter is shown to give better results in many cases 
\cite{relu_best_ever}, but is not suitable as the final layer activation function. Sigmoidal nonlinear activation functions have been shown to 
cause vanishing gradients, as discussed in the backpropacation subsection \ref{sssec:backpropagation}, but they are the golden standard as the final activation function. 
\\
\\
In our network, we choose between five different activation functions as presented below: 
\\
\\
1. Sigmoid:
The sigmoidal or logistic activation function is a saturating function that was previously used for all layers in early neural networks, 
however it's now more common as the final activation layer in binary classification networks.
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]
\\
\\
2. Softmax (for matrix input):
The softmax activation functions has the same use-case as the sigmoidal, but for non-binary classification cases. For a matrix \( z \) with rows representing sets of scores, the softmax function can be applied to each row \( z_{i,:} \) as:
\[
\text{softmax}(z_{i,j}) = \frac{e^{z_{i,j} - \max(z_{i,:})}}{\sum_{k} e^{z_{i,k} - \max(z_{i,:})}}
\]
where \( \max(z_{i,:}) \) is the maximum value in the \( i \)-th row.
\\
\\
3. ReLU (Rectified Linear Unit):
The ReLU activation function is a simple non-saturating activation function that is useful to prevent gradients from vanishing during backpropagation, 
but it can also suffer from dead neurons when the output is 0, which makes it hard to update the weights during backpropagation.  
\[
\text{ReLU}(z) = 
\begin{cases} 
   z & \text{if } z > 0 \\
   0 & \text{otherwise}
\end{cases}
\]
\\
4. Leaky ReLU:
The leaky ReLU is a version of the ReLU above which was developed by A.L.Maas, A.Y.Hannum and A.Y.Ng \cite{relu_best_ever}. The small $\delta$ coefficient avoids
setting each neuron to an absolute 0, which dealss the issue of dead neurons during backpropagation.
\[
\text{Leaky\_ReLU}(z) = 
\begin{cases} 
   z & \text{if } z > 0 \\
   \delta \cdot z & \text{otherwise}
\end{cases}
\]
\\
3. ReLU6:
We added the option to implement ReLU 6 after overflow errors during training with the BC data set. This activation
function is similar to ReLU, but with a max value of 6. 
\[
\text{ReLU6}(z) = 
\begin{cases} 
   z & \text{if } z \in \ <0, 6> \\
   0 & \text{if } z < 0 \\
   6 & \text{if } z > 6
\end{cases}
\]
\\


\subsubsection{Loss functions}\label{sssec:loss_functions}
Loss functions in neural networks are key to training the weights and biases in a network, as they are the 
initial function we aim to minimize. 
Since the whole backpropagation step is essentially built around the derivative of our loss function,
we provide a short summary of our choice of 
loss functions and where we've applied them. 
\\
\\
1. Mean Squared Error (MSE):
We use the mean square error as the loss function in all of our regression cases to compare how the targets differ from the predictions, such
as with the Topographical data preditions. 
   \[
   \text{MSE}(\hat{y}, y) = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
   \]
   where:
   - \( \hat{y} \) are the predicted values.
   - \( y \) are the target values.
   - \( n \) is the number of data points.
\\
\\
2. Cross-Entropy:
We use cross-entropy (CE) as the loss function for all classification cases with more than two outcomes (non-binary). Although none of our results include
details on this, the ANN was tested on the Iris dataset with a cross-entropy loss function. 
   \[
   \text{CE}(\hat{y}, y) = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
   \]
   where \( y \) is the target and \( \hat{y} \) is the prediction .
   %
\\
\\
3. Binary Cross-Entropy:
We use binary cross entropy (bce) as the loss function for all binary classification cases, such as the Wisconsin Breast Cancer predictions. 
   \[
   \text{BCE}(\hat{y}, y) = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
   \]
%
where \( y \) is the binary target and \( \hat{y} \) is the predicted probability.
\\

\subsubsection{Forward propagation}\label{sssec:forward_propagation}


The feed-forward process begins with an input vector \( \mathbf{x} \), which is transformed through each layer 
of the neural network by a series of transformations described below

For each layer \( l \), we compute a hidden layer \( \mathbf{h}^{(l)} \) with the following transformation:

\begin{equation}\label{eq:ffnn}
    \mathbf{h}^{(l)} = a \Big( \mathbf{W}^{(l)T} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)} \Big) ,
\end{equation}

where \( a \) is the activation function we choose as described in Subsection \ref{sssec:activation_functions}, 
\( \mathbf{W}^{(l)} \) is the weight matrix for the \( l \)-th layer, \( \mathbf{b}^{(l)} \) is the bias vector, 
and \( \mathbf{a}^{(l-1)} \) is the result of the activation function from the previous layer. 
\( \mathbf{h}^{(l)} \) is then used as the input to the next layer, becoming \( \mathbf{a}^{(l)} \).

This continues until the final layer, where the output of the last layer,
 \( \mathbf{a}^{(L)} \), is the network prediction. 

Thus, the feed-forward process systematically applies each layer’s weights, biases, and activation function 
to the input, resulting in a transformed output that serves as the final prediction from the neural network.

\subsubsection{Back-propagation}\label{sssec:backpropagation}

Back-propagation refers to the process of calculating the gradient in each layer of the neural network when
moving backwards from the final layer $L$. The gradient is found by taking the derivative of the 
loss function with regards to the weights and biases of each layer. However, each layer $l$ is connected to
it's former layer $l-1$. Therefore, the back-propagation becomes a large calculation using the chain rule of 
derivatives as illustrated below.
\\
\\
We wish to obtain the derivative of the cost function: 

$$
\delta_j^l =\frac{\partial {\cal C}}{\partial z_j^l}.
$$
%
Which can be written as
$$
\delta_j^l =\sum_k \frac{\partial {\cal C}}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^{l}}=\sum_k \delta_k^{l+1}\frac{\partial z_k^{l+1}}{\partial z_j^{l}},
$$
this is equal to
$$
\delta_j^l =\sum_k \delta_k^{l+1}w_{kj}^{l+1}\sigma'(z_j^l).
$$
\\
\\
After computing $\delta$ we can update the weights and biases as follows:
$$
w_{ij}^l\leftarrow  = w_{ij}^l- \eta \delta_j^la_i^{l-1},
$$
$$
b_j^l \leftarrow b_j^l-\eta \frac{\partial {\cal C}}{\partial b_j^l}=b_j^l-\eta \delta_j^l,
$$
%
where $\eta$ is our learning rate.
\\
\\
Doing this step manually can quickly become a time consuming task. We chose to instead implement an automatic 
differentiation step, as desribed in our Tools subsection\ref{ssec:tools}.



%------------ Data --------------------------------------------
\subsection{Data}\label{ssec:data}
%
\subsubsection*{Topographical data}
We obtained geographical data of the Stavanger area, Norway, from EarthExplorer (\footnote{\url{https://earthexplorer.usgs.gov/}}, via \footnote{\url{https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2023/Project1/DataFiles}}). 
The data contained a 200x200 grid of altitudes (m) of the area, with arbitrary x- and y-coordinates that we used as our features. 
We split the data in 80:20 training 
and data set using \texttt{train\_test\_split} and scaled by standard normalization. We chose to scale since some linear regression 
models such as Lasso and Ridge perform better when all features are on a similar scale \cite{raschka:2022:ml_pytorch_scikit}. 
\\
\subsubsection*{Wisconsin breast cancer data}
The Wisconsin Breast Cancer data \cite{bc_wisconsin} is considered to be a benchmark data set for testing novel or existing machine learning methods on, and there is extensive documentation on previously explored machine learning methods on the data that can be found here: 
\cite{wisconsin_example1}, \cite{wisconsin_example2} and \cite{wisconsin_example3}.

As described in the original article which produced the data \cite{first_wisconsin}, it consists of a 30 features, 29 of which are describing biopsied cell nuclei from 
a total of 569 samples and the final feature is the diagnosis we wish to predict. We treated the data just as with the topographical data, with \texttt{train\_test\_split} as well as standard normalization. 

To produce the data, non-invasive fine needle aspirations of tumors were put on glass and stained, then pictures were taken of each sample. The features were then derived from each image with a "computer diagnostic system" that 
analyzed the images and computed an extensive set of nuclei features such as radius, perimeter, area, smoothness, concavity, symmetry and texture. Some insight into the data can be viewed in the \texttt{Cancer\_appendix.html} found in our Github repository. 

The authors Nick Street and William H. Wolberg from the original paper performed a classification using a variant on the Multisurface 
Method, where they reached an accuracy of 97$\%$ after a ten-fold cross validation process with a sensitivity of 0.9 and a specificity of 0.86. 
These terms are important to include in any analysis on medicinal data as the implementation of computer aided diagnosis in clinical practice 
needs to be thoroughly documented and understood by physicians beforehand: 
\\
\\
Sensitivity = $\frac{correct\ positive}{total\ positive}$,
\\
\\
Specificity = $\frac{correct\ negative}{total\ negative}$.
%
%------------ Tools -------------------------------------------
\subsection{Tools}\label{ssec:tools}
The models were implemented in \verb|Python| version 3.12, and the figures produced using the \verb|matplotlib| library, 
and stylized using \verb|seaborn|. The FFNN was implemented from scratch, using the \verb|autograd| library \cite{maclaurin2015:autograd} 
for computing the gradients. We used the general modeling tools from \verb|scikit-learn| \cite{scikit-learn} to test our models, and validated our implementations of logistic regression and neural network classification against the implementations there.