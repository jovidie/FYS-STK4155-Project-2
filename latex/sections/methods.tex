%==============================================================
\section{Methods}\label{sec:methods}
% Describe the methods and algorithms used, include any formulas. 
% Explain how everything is implemented, and possibly mention the structure of the algorithm. 
% Add demonstrations such as tests, selected runs and validations. 
%==============================================================
Janita, Even writes about GD and SGD?
%------------ Background? -------------------------------------
\subsection{Regression vs. Classification}\label{ssec:regression_classification}


%------------ Logistic Regression -----------------------------
\subsection{Logistic Regression}\label{ssec:logreg}
Logistic regression is a method of classification, which estimates the probability of being in a certain class. In contrast to linear regression methods, where the outcome is continuous function, the outcome of logistic regression is a linear classifier which gives a decision boundary between classes. This method is often used as a baseline model, particularly in problems in the nature of binary classification, which is what we will focus on.

The sigmoid function is used to fit the model, and is written as
\begin{equation}\label{eq:sigmoid}
    p(z) = \frac{1}{1 + \exp{-z}} , 
\end{equation}
where $z$ is the model's prediction. We define the cost function as the log-likelihood in Equation \eqref{eq:log_likelihood}, which is derived from the Maximum Likelihood Principle.
\begin{equation}\label{eq:log_likelihood}
    \mathcal{C}(\mathbf{\beta}) = - \sum_{i=1}^{n} (y_{i} (\beta_{0} + \beta_{1} x_{i}) - \log (1 + \exp{\beta_{0} + \beta_{1} x_{i}})))
\end{equation}


%------------ Gradient Descent --------------------------------
\subsection{Gradient Descent}\label{ssec:gradient_descent}

%% work-in-progress her, m책 samle tankene for 책 forklare dette p책 en god m책te!
For both regression and classification, we want to optimize a set of parameters $\theta$ given a cost function $C(X, \theta)$. This is ususally done by minimizing the cost function. One way of finding the minimum of the cost function given our parameters is by gradient descent (GD) (Algorithm \ref{alg:gd}). In gradient descent you start with a random set of parameters, and change these in small steps towards the optimal values by moving iteratively along a gradient \cite{Goodfellow:2016:deep_learning}. The gradient is found by calculating the first derivatives of the cost function with respect to the parameters, and evaluating these for each iteration. The rate of descent for each parameter is ideally determined using Newton's method, but this requires the often prohibitively expensive operation of calculating the Hessian matrix \cite{battiti1992:newtons_method}. Instead, it is common use a fixed step size $\eta$, known as the learning rate of the model. The algorithm is run either until some convergence criterion is reached (e.g., gradients approaching 0) it reaches the maximum number of iterations.

\begin{algorithm}
\caption{Gradient descent}\label{alg:gd}
\begin{algorithmic}[1]
    \STATE Initialize parameters $\theta = \theta_0$
    \STATE Choose a learning rate $\eta > 0$
    \STATE Set number of iterations K
    \FOR{iteration $=1$ to K}
        \STATE Compute the gradient $\nabla C(\theta)$
        \STATE Update the parameters: $\theta \leftarrow \theta - \eta \nabla C(\theta)$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

While gradient descent effectively minimizes the cost function given the starting parameters, the algorithm can find a local minimum, rather than the lower global minimum. Additionally, gradient descent is computationally expensive for large data sets with many features. It is common to instead use a small random subset of your data each time you compute the gradients, rather than the full data, known as stochastic gradient descent (SGD, also known as mini-batch gradient descent) (Algorithm \ref{alg:sgd}). SGD increases the chance of avoiding local minima, and is more computationally efficient than GD \cite{Goodfellow:2016:deep_learning}.

\begin{algorithm}
\caption{Stochastic gradient descent with mini-batches}\label{alg:sgd}
\begin{algorithmic}[1]
    \STATE Initialize parameters $\theta = \theta_0$
    \STATE Choose a learning rate $\eta > 0$
    \STATE Choose mini-batch size $m$
    \STATE Set number of epochs $K$
    \FOR{epoch $= 1$ to $K$}
        \STATE Shuffle the training data
        \FOR{each mini-batch \( \mathcal{B}_i \) of size $m$}
            \STATE Compute the gradient: $\frac{1}{m}\nabla_{\mathcal{B}_i} C(\theta)$
            \STATE Update the parameters: $\theta \leftarrow \theta - \eta \nabla_{\mathcal{B}_i} C(\theta)$
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Rather than simply using the current gradient and a fixed learning rate $\eta$, it is common to also use information from the previous step when determining the size and direction of the current step. One way to do this is by adding the previous step along the gradient multiplied with a constant $\gamma$ to the current gradient step, as
\begin{equation}
    \mathbf{v}_{t} = \eta \nabla C(\boldsymbol{\theta}) + \gamma \mathbf{v}_{t-1}
\end{equation}
\begin{equation}
    \boldsymbol{\theta}_{t+1}= \boldsymbol{\theta}_t -\mathbf{v}_{t},
\end{equation}
    
where $\theta$ is the parameters, and $\mathbf{v}_t$ and $\mathbf{v}_{t-1}$ is the current and previous change in the gradient, respectively. This method is known as gradient descent with momentum, and can be used with both GD and SGD. Adding momentum changes both the direction and magnitude of the steps based on the previous gradients, and often improves convergence times \cite{Goodfellow:2016:deep_learning}.

As the optimal learning rate for reaching a minimum often changes as you iterate, several algorithms (optimizers) exist to adaptively change the learning rate during gradient descent. Three common optimizers are AdaGrad \cite{duchi2011:adagrad}, RMSProp \cite{hinton2012:rmsprop}, and ADAM \cite{kingma2017:adam}. These methods all scale the learning rate using the accumulated square gradients during the course of training. While AdaGrad simply divides the learning rate by the square root of the summed square gradients, RMSProp implements a decay rate so the more recent gradients have a larger contribution than earier ones. ADAM can be seen as an implementation of RMSProp with momentum, but that also rescales the gradients and applies bias-correction before scaling the learning rate \cite{Goodfellow:2016:deep_learning}. RMSProp and ADAM tends to outperform AdaGrad, and is frequently used in machine learning \cite{Goodfellow:2016:deep_learning}.




%------------ Feed-Forward Neural Network ---------------------
\subsection{Feed-Forward Neural Network}\label{ssec:ffnn}
In recent years, neural networks have shown promise in solving both regression and classification problems. They and have evolved into several types of networks, with the simplest one called feed-forward neural network (FFNN). In FFNNs, information moves through the layers in one direction, and the network is said to be fully connected if each neuron in a layer is connected to all neurons in the next layer, illustrated in Figure \ref{fig:ffnn}. 

\begin{figure}[h!]
    \centering
    \resizebox{0.9\linewidth}{!}
    {\input{latex/sections/tikz/ffnn}}
    %\includegraphics[width=0.5\linewidth]{}
    \caption{Illustration of a feed-forward neural network with one input layer, three hidden layers, and one output layer, where $n$, $m$ and $k$ indicate the number of neurons in the respective layer.}
    \label{fig:ffnn}
\end{figure}
The architecture of a neural network is often determined by the problem to be solved. According to the universal approximation theorem, a FFNN with a minimum of one input layer, one hidden layer with a non-linear activation function, and one linear output layer, is sufficient to approximate a continuous function \cite[194]{Goodfellow:2016:deep_learning}. 

The output of a hidden layer can be written as 
\begin{equation}\label{eq:ffnn}
    \mathbf{h} = a \Big( \mathbf{W}^{T} \mathbf{x} + \mathbf{b} \Big) ,
\end{equation}
where $a$ is a non-linear activation function, $\mathbf{W}$ is the weight matrix, $\mathbf{x}$ is input, and $\mathbf{b}$ is the bias. 

\subsubsection{Weights and biases}\label{sssec:weights_biases}
The technique used to initialize the weights and biases in the FFNN can be vital for how fast the network learns. For a less ideal method, the network may need more iterations to find an optimal solution. Whereas a clever initialization may reduce the number of iteration needed, as the weight are some of the hyper-parameters that needs tuning. 

\subsubsection{Activation functions}\label{sssec:activation_functions}


\subsubsection{Cost functions}\label{sssec:cost_functions}


\subsubsection{Forward propagation}\label{sssec:forward_propagation}


\subsubsection{Back-propagation}\label{sssec:backpropagation}


%------------ Data --------------------------------------------
\subsection{Data}\label{ssec:data}


%------------ Tools -------------------------------------------
\subsection{Tools}\label{ssec:tools}
The models were implemented in \verb|Python| version 3.12, and the figures produced using the \verb|matplotlib| library, and stylized using \verb|seaborn|. The FFNN was implemented from scratch, using the \verb|autograd| library \cite{maclaurin2015:autograd} for computing the gradients.