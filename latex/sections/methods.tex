%==============================================================
\section{Methods}\label{sec:methods}
% Describe the methods and algorithms used, include any formulas. 
% Explain how everything is implemented, and possibly mention the structure of the algorithm. 
% Add demonstrations such as tests, selected runs and validations. 
%==============================================================
Janita, Even writes about GD and SGD?
%------------ Background? -------------------------------------
\subsection{Regression vs. Classification}\label{ssec:regression_classification}


%------------ Logistic Regression -----------------------------
\subsection{Logistic Regression}\label{ssec:logreg}
Logistic regression is a method of classification, which estimates the probability of being in a certain class. In contrast to linear regression methods, where the outcome is continuous function, the outcome of logistic regression is a linear classifier which gives a decision boundary between classes. This method is often used as a baseline model, particularly in problems in the nature of binary classification, which is what we will focus on.

The sigmoid function is used to fit the model, and is written as
\begin{equation}\label{eq:sigmoid}
    p(z) = \frac{1}{1 + \exp{-z}} , 
\end{equation}
where $z$ is the model's prediction. We define the cost function as the log-likelihood in Equation \eqref{eq:log_likelihood}, which is derived from the Maximum Likelihood Principle.
\begin{equation}\label{eq:log_likelihood}
    \mathcal{C}(\mathbf{\beta}) = - \sum_{i=1}^{n} (y_{i} (\beta_{0} + \beta_{1} x_{i}) - \log (1 + \exp{\beta_{0} + \beta_{1} x_{i}})))
\end{equation}


%------------ Gradient Descent --------------------------------
\subsection{Gradient Descent}\label{ssec:gradient_descent}

%% work-in-progress her, m책 samle tankene for 책 forklare dette p책 en god m책te!
For both regression and classification, we want to optimize a set of parameters $\theta$ given a cost function $C(X, \theta)$. This is ususally done by minimizing the cost function. One way of finding the minimum of the cost function given our parameters is by gradient descent (GD) (Algorithm \ref{alg:gd}). In gradient descent you start with a random set of parameters, and change these in small steps towards the optimal values by moving iteratively along a gradient \cite{Goodfellow:2016:deep_learning}. The gradient is found by calculating the first derivatives of the cost function with respect to the parameters, and evaluating these for each iteration. The rate of descent for each parameter is ideally determined using Newton's method, but this requires the often prohibitively expensive operation of calculating the Hessian matrix \cite{battiti1992:newtons_method}. Instead, it is common use a fixed step size $\eta$, known as the learning rate of the model. The algorithm is run either until some convergence criterion is reached (e.g., gradients approaching 0) it reaches the maximum number of iterations.

\begin{algorithm}
\caption{Gradient descent}\label{alg:gd}
\begin{algorithmic}[1]
    \STATE Initialize parameters $\theta = \theta_0$
    \STATE Choose a learning rate $\eta > 0$
    \STATE Set number of iterations K
    \FOR{iteration $=1$ to K}
        \STATE Compute the gradient $\nabla C(\theta)$
        \STATE Update the parameters: $\theta \leftarrow \theta - \eta \nabla C(\theta)$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

While gradient descent effectively minimizes the cost function given the starting parameters, the algorithm can find a local minimum, rather than the lower global minimum. Additionally, gradient descent is computationally expensive for large data sets with many features. It is common to instead use a small random subset of your data each time you compute the gradients, rather than the full data, known as stochastic gradient descent (SGD, also known as mini-batch gradient descent) (Algorithm \ref{alg:sgd}). SGD increases the chance of avoiding local minima, and is more computationally efficient than GD \cite{Goodfellow:2016:deep_learning}.

\begin{algorithm}
\caption{Stochastic gradient descent with mini-batches}\label{alg:sgd}
\begin{algorithmic}[1]
    \STATE Initialize parameters $\theta = \theta_0$
    \STATE Choose a learning rate $\eta > 0$
    \STATE Choose mini-batch size $m$
    \STATE Set number of epochs $K$
    \FOR{epoch $= 1$ to $K$}
        \STATE Shuffle the training data
        \FOR{each mini-batch \( \mathcal{B}_i \) of size $m$}
            \STATE Compute the gradient: $\frac{1}{m}\nabla_{\mathcal{B}_i} C(\theta)$
            \STATE Update the parameters: $\theta \leftarrow \theta - \eta \nabla_{\mathcal{B}_i} C(\theta)$
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Rather than simply using the current gradient and a fixed learning rate $\eta$, it is common to also use information from the previous step when determining the size and direction of the current step. One way to do this is by adding the previous step along the gradient multiplied with a constant $\gamma$ to the current gradient step, as
\begin{equation}
    \mathbf{v}_{t} = \eta \nabla C(\boldsymbol{\theta}) + \gamma \mathbf{v}_{t-1}
\end{equation}
\begin{equation}
    \boldsymbol{\theta}_{t+1}= \boldsymbol{\theta}_t -\mathbf{v}_{t},
\end{equation}
    
where $\theta$ is the parameters, and $\mathbf{v}_t$ and $\mathbf{v}_{t-1}$ is the current and previous change in the gradient, respectively. This method is known as gradient descent with momentum, and can be used with both GD and SGD. Adding momentum changes both the direction and magnitude of the steps based on the previous gradients, and often improves convergence times \cite{Goodfellow:2016:deep_learning}.

As the optimal learning rate for reaching a minimum often changes as you iterate, several algorithms (optimizers) exist to adaptively change the learning rate during gradient descent. Three common optimizers are AdaGrad \cite{duchi2011:adagrad}, RMSProp \cite{hinton2012:rmsprop}, and ADAM \cite{kingma2017:adam}. These methods all scale the learning rate using the accumulated square gradients during the course of training. While AdaGrad simply divides the learning rate by the square root of the summed square gradients, RMSProp implements a decay rate so the more recent gradients have a larger contribution than earier ones. ADAM can be seen as an implementation of RMSProp with momentum, but that also rescales the gradients and applies bias-correction before scaling the learning rate \cite{Goodfellow:2016:deep_learning}. RMSProp and ADAM tends to outperform AdaGrad, and is frequently used in machine learning \cite{Goodfellow:2016:deep_learning}.




%------------ Feed-Forward Neural Network ---------------------
\subsection{Feed-Forward Neural Network}\label{ssec:ffnn}
In recent years, neural networks have shown promise in solving both regression and classification problems. They and have evolved into several types of networks, with the simplest one called feed-forward neural network (FFNN). In FFNNs, information moves through the layers in one direction, and the network is said to be fully connected if each neuron in a layer is connected to all neurons in the next layer, illustrated in Figure \ref{fig:ffnn}. 

\begin{figure}[h!]
    \centering
    \resizebox{0.9\linewidth}{!}
    {\input{latex/sections/tikz/ffnn}}
    %\includegraphics[width=0.5\linewidth]{}
    \caption{Illustration of a feed-forward neural network with one input layer, three hidden layers, and one output layer, where $n$, $m$ and $k$ indicate the number of neurons in the respective layer.}
    \label{fig:ffnn}
\end{figure}
The architecture of a neural network is often determined by the problem to be solved. According to the universal approximation theorem, a FFNN with a minimum of one input layer, one hidden layer with a non-linear activation function, and one linear output layer, is sufficient to approximate a continuous function \cite[194]{Goodfellow:2016:deep_learning}. 

The output of a hidden layer can be written as 
\begin{equation}\label{eq:ffnn}
    \mathbf{h} = a \Big( \mathbf{W}^{T} \mathbf{x} + \mathbf{b} \Big) ,
\end{equation}
where $a$ is a non-linear activation function, $\mathbf{W}$ is the weight matrix, $\mathbf{x}$ is input, and $\mathbf{b}$ is the bias. 

\subsubsection{Weights and biases}\label{sssec:weights_biases}
The technique used to initialize the weights and biases in the FFNN can be vital for how fast the network learns. For a less ideal method, the network may need more iterations to find an optimal solution. Whereas a clever initialization may reduce the number of iteration needed, as the weight are some of the hyper-parameters that needs tuning. 

\subsubsection{Activation functions}\label{sssec:activation_functions}

Activation functions in neural networks are meant to represent neuronal firing, a process which in simplified terms is either on or off. Thus, the output 
of an activation function is going to be between 0 and 1, depending on the input. If 0, the neuron can be considered "dead" and will not contribute 
to the final prediction. If 1, the neuron is completely "on" and will pass on the information to the next node as described in Eq. (\cite{ffnn}). 

Here we separate between sigmoidal nonlinearities and rectified linear hidden units (ReLU). The latter is shown to give better results in many cases 
\cite{relu_best_ever}, but is not suitable as the final layer activation function. Sigmoidal nonlinear activation functions have been shown to 
cause vanishing gradients, as discussed in \ref{subsec:backpropagation}, but they are the golden standard as the final activation function. 
\\
\\
In our network, we choose between four different activation functions as presented below: 
\\
\\
1. \textbf{Sigmoid:}
The sigmoidal or logistic activation function is a saturating function that was previously used for all layers in early neural networks, 
however it's now more common as the final activation layer in binary classification networks.
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]
\\
\\
2. \textbf{Softmax (for matrix input):}
The softmax activation functions has the same use-case as the sigmoidal, but for non-binary classification cases. 

For a matrix \( z \) with rows representing sets of scores, the softmax function can be applied to each row \( z_{i,:} \) as:
\[
\text{softmax}(z_{i,j}) = \frac{e^{z_{i,j} - \max(z_{i,:})}}{\sum_{k} e^{z_{i,k} - \max(z_{i,:})}}
\]
where \( \max(z_{i,:}) \) is the maximum value in the \( i \)-th row.
\\
\\
3. \textbf{ReLU (Rectified Linear Unit):}
The ReLU activation function is a simple non-saturating activation function that is useful to prevent gradients from vanishing during backpropagation, 
but it can also suffer from dead neurons when the output is 0, which makes it hard to update the weights during backpropagation.  
\[
\text{ReLU}(z) = 
\begin{cases} 
   z & \text{if } z > 0 \\
   0 & \text{otherwise}
\end{cases}
\]
\\
4. \textbf{Leaky ReLU:}
The leaky ReLU is a version of the ReLU above which was developed by A.L.Maas, A.Y.Hannum and A.Y.Ng \cite{relu_best_ever}. The $\delta$ coefficient avoids
setting each neuron to an absolute 0, which dealss the issue of dead neurons during backpropagation.
\[
\text{Leaky\_ReLU}(z) = 
\begin{cases} 
   z & \text{if } z > 0 \\
   \delta \cdot z & \text{otherwise}
\end{cases}
\]


\subsubsection{Loss functions}\label{sssec:loss_functions}
Loss functions in neural networks are key to training the weights and biases in a network, as they are the initial function we aim to minimize. 
Since the whole backpropagation step is essentially built around the derivative of our loss function, we provide a short summary of our choice of 
loss functions and where we've applied them. 
\\
\\
1. \textbf{Mean Squared Error (MSE):}
We use the mean square error as the loss function in all of our regression cases to compare how the targets differ from the predictions, such
as with the Topographical data preditions. 
   \[
   \text{MSE}(\hat{y}, y) = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
   \]
   where:
   - \( \hat{y} \) are the predicted values.
   - \( y \) are the target values.
   - \( n \) is the number of data points.
\\
\\
2. \textbf{Cross-Entropy:}
We use cross-entropy (CE) as the loss function for all classification cases with more than two outcomes (non-binary). Although none of our results include
details on this, the ANN was tested on the Iris dataset with a cross-entropy loss function. 
   \[
   \text{CE}(\hat{y}, y) = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)
   \]
   where \( y \) is the target and \( \hat{y} \) is the prediction .
   %
\\
\\
3. \textbf{Binary Cross-Entropy:}
We use binary cross entropy (bce) as the loss function for all binary classification cases, such as the Wisconsin Breast Cancer predictions. 
   \[
   \text{BCE}(\hat{y}, y) = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
   \]
%
where \( y \) is the binary target and \( \hat{y} \) is the predicted probability.
\\

\subsubsection{Forward propagation}\label{sssec:forward_propagation}

\subsubsection{Back-propagation}\label{sssec:backpropagation}

Something about vanishing gradients...


%------------ Data --------------------------------------------
\subsection{Data}\label{ssec:data}
%
\paragraph*{Topographical data}
We obtained geographical data of the Stavanger area, Norway, from EarthExplorer (\footnote{\url{https://earthexplorer.usgs.gov/}}, 
via \footnote{\url{https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2023/Project1/DataFiles}}). 
The data contained a 200x200 grid of altitudes (m) of the area, with arbitrary x- and y-coordinates that we used as our features. 
We constructed a design matrix with polynomial features in the same manner as with synthetic data, and split the data in 80:20 training 
and data set using \texttt{train\_test\_split}. We scaled the data by standard normalization. We chose to scale since some linear regression 
models such as Lasso and Ridge perform better when all features are on a similar scale \supercite{raschka2019}. 
\\
\paragraph*{Wisconsin breast cancer data}
The Wisconsin data set \cite{bc_wisconsin} is considered to be a benchmark data set for testing novel or existing machine learning methods on, and 
there is extensive documentation on previously explored machine learning methods on the data that can be found here: 
\cite{wisconsin_example1}, \cite{wisconsin_example2} and \cite{wisconsin_example3}.

As described in the original article which produced the data \cite{first_wisconsin}, it consists of a 30 features describing biopsied cell nuclei from 
a total of 569 samples. To produce the data, non-invasive fine needle aspirations of tumors were put on glass and stained, 
then pictures were taken of each sample. The features were then derived from each image with a "computer diagnostic system" that 
analyzed the images and computed an extensive set of nuclei features such as radius, perimeter, area, smoothness, concavity, symmetry and texture. 

The authors Nick Street and William H. Wolberg from the original paper performed a classification using a variant on the Multisurface 
Method, where they reached an accuracy of 97$\%$ after a ten-fold cross validation process with a sensitivity of 0.9 and a specificity of 0.86. 
These terms are important to include in any analysis on medicinal data as the implementation of computer aided diagnosis in clinical practice 
needs to be thoughourly documented and understood by physicians beforehand. Sensitivity is described as $\frac{correct\ positive}{total\ positive}$, 
while the specificity is $\frac{correct\ negative}{total\ negative}$
%
%------------ Tools -------------------------------------------
\subsection{Tools}\label{ssec:tools}
The models were implemented in \verb|Python| version 3.12, and the figures produced using the \verb|matplotlib| library, and stylized using \verb|seaborn|. The FFNN was implemented from scratch, using the \verb|autograd| library \cite{maclaurin2015:autograd} for computing the gradients.