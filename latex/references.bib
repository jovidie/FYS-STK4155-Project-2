%================================================================
%------------------------- Books --------------------------------
%================================================================
@book{hastie:2009:elements,
    author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	title = {The {Elements} of {Statistical} {Learning}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2024-09-10},
	publisher = {Springer New York},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
}

@book{Goodfellow:2016:deep_learning,
    title   = {Deep Learning},
    author  = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher = {MIT Press},
    url     = {http://www.deeplearningbook.org},
    year    = {2016}
}


@book{raschka:2022:ml_pytorch_scikit,
	title  = {Machine learning with {PyTorch} and {Scikit}-{Learn}: develop machine learning and deep learning models with {Python}},
	author = {Raschka, Sebastian and Liu, Yuxi and Mirjalili, Vahid and Dzhulgakov, Dmytro},
    publisher = {Packt},
	year   = {2022},
}


%================================================================
%------------------------- Articles -----------------------------
%================================================================

@article{wisconsin_example1,
  title={Machine learning in medicine: a practical introduction},
  author={Jenni A. M. Sidey-Gibbons and Chris J. Sidey-Gibbons},
  journal={BMC Medical Research Methodology},
  year={2019},
  volume={19},
  url={https://api.semanticscholar.org/CorpusID:84183143}
}

@article{wisconsin_example2,
  title={A novel feature selection approach for biomedical data classification},
  author={Yonghong Peng and Zhi Qing Wu and Jianmin Jiang},
  journal={Journal of biomedical informatics},
  year={2010},
  volume={43 1},
  pages={
          15-23
        },
  url={https://api.semanticscholar.org/CorpusID:1710466}
}

@inproceedings{wisconsin_example3,
  author={Walter, D. and Mohan, C.K.},
  booktitle={Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)}, 
  title={ClaDia: a fuzzy classifier system for disease diagnosis}, 
  year={2000},
  volume={2},
  number={},
  pages={1429-1435 vol.2},
  keywords={Fuzzy systems;Diseases;Fuzzy sets;Computer science;Evolutionary computation;Breast cancer;Expert systems;System testing;Machine learning;Humans},
  doi={10.1109/CEC.2000.870821}}

@inproceedings{relu_best_ever,
    title={Rectifier Nonlinearities Improve Neural Network Acoustic Models},
    author={Andrew L. Maas},
    booktitle = {Proceedings of the 30th International Conference on Machine Learning, Vol. 28, 3.},
    year={2013},
    url={https://api.semanticscholar.org/CorpusID:16489696}
}

@inproceedings{first_wisconsin,
  title={Nuclear feature extraction for breast tumor diagnosis},
  author={William Nick Street and William H. Wolberg and Olvi L. Mangasarian},
  booktitle={Electronic imaging},
  year={1993},
  url={https://api.semanticscholar.org/CorpusID:14922543}
}

@article{turing_36,
 ISSN = {00264423, 14602113},
 URL = {http://www.jstor.org/stable/2251299},
 author = {A. M. Turing},
 journal = {Mind},
 number = {236},
 pages = {433--460},
 publisher = {[Oxford University Press, Mind Association]},
 title = {Computing Machinery and Intelligence},
 volume = {59},
 year = {1950}
}

@ARTICLE{heman,
  title        = "Delving deep into rectifiers: Surpassing human-level
                  performance on {ImageNet} classification",
  author       = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                  Jian",
  abstract     = "Rectified activation units (rectifiers) are essential for
                  state-of-the-art neural networks. In this work, we study
                  rectifier neural networks for image classification from two
                  aspects. First, we propose a Parametric Rectified Linear Unit
                  (PReLU) that generalizes the traditional rectified unit.
                  PReLU improves model fitting with nearly zero extra
                  computational cost and little overfitting risk. Second, we
                  derive a robust initialization method that particularly
                  considers the rectifier nonlinearities. This method enables
                  us to train extremely deep rectified models directly from
                  scratch and to investigate deeper or wider network
                  architectures. Based on our PReLU networks (PReLU-nets), we
                  achieve 4.94\% top-5 test error on the ImageNet 2012
                  classification dataset. This is a 26\% relative improvement
                  over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our
                  knowledge, our result is the first to surpass human-level
                  performance (5.1\%, Russakovsky et al.) on this visual
                  recognition challenge.",
  year         =  2015,
  primaryClass = "cs.CV",
  eprint       = "1502.01852"
}

@ARTICLE{mccu_pitt,
  title     = "A logical calculus of the ideas immanent in nervous activity",
  author    = "McCulloch, Warren S and Pitts, Walter",
  abstract  = "Because of the ``all-or-none'' character of nervous activity,
               neural events and the relations among them can be treated by
               means of propositional logic. It is found that the behavior of
               every net can be described in these terms, with the addition of
               more complicated logical means for nets containing circles; and
               that for any logical expression satisfying certain conditions,
               one can find a net behaving in the fashion it describes. It is
               shown that many particular choices among possible
               neurophysiological assumptions are equivalent, in the sense that
               for every net behaving under one assumption, there exists
               another net which behaves under the other and gives the same
               results, although perhaps not in the same time. Various
               applications of the calculus are discussed.",
  journal   = "Bulletin of Mathematical Biophysics",
  publisher = "Springer Science and Business Media LLC",
  volume    =  5,
  number    =  4,
  pages     = "115--133",
  month     =  dec,
  year      =  1943,
  language  = "en"
}

@article{histopath_AI,
  title = {Deep learning in histopathology: the path to the clinic},
  volume = {27},
  ISSN = {1546-170X},
  url = {http://dx.doi.org/10.1038/s41591-021-01343-4},
  DOI = {10.1038/s41591-021-01343-4},
  number = {5},
  journal = {Nature Medicine},
  publisher = {Springer Science and Business Media LLC},
  author = {van der Laak,  Jeroen and Litjens,  Geert and Ciompi,  Francesco},
  year = {2021},
  month = may,
  pages = {775â€“784}
}


@InProceedings{xaviergloriot,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@ARTICLE{best_bias,
  title     = "A mathematical framework for improved weight initialization of
               neural networks using Lagrange multipliers",
  author    = "de Pater, Ingeborg and Mitici, Mihaela",
  abstract  = "A good weight initialization is crucial to accelerate the
               convergence of the weights in a neural network. However,
               training a neural network is still time-consuming, despite
               recent advances in weight initialization approaches. In this
               paper, we propose a mathematical framework for the weight
               initialization in the last layer of a neural network. We first
               derive analytically a tight constraint on the weights that
               accelerates the convergence of the weights during the
               back-propagation algorithm. We then use linear regression and
               Lagrange multipliers to analytically derive the optimal initial
               weights and initial bias of the last layer, that minimize the
               initial training loss given the derived tight constraint. We
               also show that the restrictive assumption of traditional weight
               initialization algorithms that the expected value of the weights
               is zero is redundant for our approach. We first apply our
               proposed weight initialization approach to a Convolutional
               Neural Network that predicts the Remaining Useful Life of
               aircraft engines. The initial training and validation loss are
               relatively small, the weights do not get stuck in a local
               optimum, and the convergence of the weights is accelerated. We
               compare our approach with several benchmark strategies. Compared
               to the best performing state-of-the-art initialization strategy
               (Kaiming initialization), our approach needs 34\% less epochs to
               reach the same validation loss. We also apply our approach to
               ResNets for the CIFAR-100 dataset, combined with transfer
               learning. Here, the initial accuracy is already at least 53\%.
               This gives a faster weight convergence and a higher test
               accuracy than the benchmark strategies.",
  journal   = "Neural Netw.",
  publisher = "Elsevier BV",
  volume    =  166,
  pages     = "579--594",
  month     =  sep,
  year      =  2023,
  keywords  = "Lagrange function; Linear regression; Neural network training;
               Remaining useful life; Weight initialization",
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}


@article{battiti1992:newtons_method,
  title = {First- and {{Second-Order Methods}} for {{Learning}}: {{Between Steepest Descent}} and {{Newton}}'s {{Method}}},
  shorttitle = {First- and {{Second-Order Methods}} for {{Learning}}},
  author = {Battiti, Roberto},
  year = {1992},
  month = mar,
  journal = {Neural Computation},
  volume = {4},
  number = {2},
  pages = {141--166},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.2.141},
  urldate = {2024-11-01},
  abstract = {On-line first-order backpropagation is sufficiently fast and effective for many large-scale classification problems but for very high precision mappings, batch processing may be the method of choice. This paper reviews first- and second-order optimization methods for learning in feedforward neural networks. The viewpoint is that of optimization: many methods can be cast in the language of optimization techniques, allowing the transfer to neural nets of detailed results about computational complexity and safety procedures to ensure convergence and to avoid numerical problems. The review is not intended to deliver detailed prescriptions for the most appropriate methods in specific applications, but to illustrate the main characteristics of the different methods and their mutual relations.},
  file = {/home/fleskelapp/Zotero/storage/C2DTF6NS/First-and-Second-Order-Methods-for-Learning.html}
}

@article{duchi2011:adagrad,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {Journal of machine learning research},
  volume = {12},
  number = {7},
  urldate = {2024-11-04},
  file = {/home/fleskelapp/Zotero/storage/3STUMG2P/Duchi et al. - 2011 - Adaptive subgradient methods for online learning a.pdf}
}

@misc{hinton2012:rmsprop,
  title = {Neural Networks for Machine Learning Lecture 6a Overview of Mini-Batch Gradient Descent},
  author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year = {2012},
  address = {Coursera},
  urldate = {2024-11-04},
  file = {/home/fleskelapp/Zotero/storage/Y4WFQKF5/Hinton et al. - 2012 - Neural networks for machine learning lecture 6a ov.pdf}
}

@misc{kingma2017:adam,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2024-11-04},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/fleskelapp/Zotero/storage/TUGRT8FZ/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/fleskelapp/Zotero/storage/C437ZFCS/1412.html}
}

@inproceedings{maclaurin2015:autograd,
  title = {Autograd: {{Effortless}} Gradients in Numpy},
  shorttitle = {Autograd},
  booktitle = {{{ICML}} 2015 {{AutoML}} Workshop},
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
  year = {2015},
  volume = {238},
  urldate = {2024-11-04},
  file = {/home/fleskelapp/Zotero/storage/HSGZ3ARH/Maclaurin et al. - 2015 - Autograd Effortless gradients in numpy.pdf}
}

@book{franke1979,
  title = {A Critical Comparison of Some Methods for Interpolation of Scattered Data},
  author = {Franke, Richard},
  year = {1979},
  publisher = {Naval Postgraduate School Monterey, CA},
  file = {C:\Users\Even\Zotero\storage\G88XI5TU\Franke - 1979 - A critical comparison of some methods for interpol.pdf}
}


%================================================================
%------------------------- Web pages ----------------------------
%================================================================

@misc{who_physicians,
	author = {},
	title = {Density of physicians (per 10 000 population)},
	howpublished = {\url{https://data.who.int/indicators/i/CCCEBB2/217795A}},
	year = {},
	note = {[Accessed 05-11-2024]},
}


%================================================================
%------------------------- Misc ---------------------------------
%================================================================

@misc{bc_wisconsin,
  author       = {Wolberg, William, Mangasarian, Olvi, Street, Nick, and Street, W.},
  title        = {{Breast Cancer Wisconsin (Diagnostic)}},
  year         = {1993},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5DW2B}
}

}