---
title: "Gradient descent exploration"
format: pdf
---

Here I test the gradient descent algorithm on a simple polynomial, before going on to using it on the Franke function and then real terrain data.

```{python}
import autograd.numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import train_test_split

from ptwo.utils import calculate_polynomial, franke_function
from ptwo.models import GradientDescent
from ptwo.optimizers import Momentum, ADAM, AdaGrad, RMSProp
from ptwo.gradients import grad_OLS, grad_ridge
from ptwo.costfuns import mse

```

# Polynomial data

Start by generating data for a polynomial expression

```{python}
np.random.seed(8923)

n = 100
x = np.linspace(-3, 2, n)
poly_list = np.array([3, 3, 1, 4])
y = calculate_polynomial(x, *poly_list) 
y = y.reshape(-1,1)

# choose polynomial degree that matches design
X = PolynomialFeatures(len(poly_list) - 1).fit_transform(x.reshape(-1, 1))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

scalerx = StandardScaler()
X_train_scaled = scalerx.fit_transform(X_train)
X_test_scaled = scalerx.transform(X_test)

scalery = StandardScaler()
y_train_scaled = scalerx.fit_transform(y_train)
y_test_scaled = scalerx.transform(y_test)
```

Then we test a standard gradient descent without optimizers. For this data generated by polynomials, we just compare the coefficients with the true values used to generate the data.

```{python}
learning_rate = 0.01
n_iter = 200
grad = grad_OLS()

gd = GradientDescent(learning_rate, grad)

gd.descend(X, y, epochs = n_iter)
print("Regular", gd.theta, sep = "\n")
```

It does not seem to converge completely after 200 iterations, let's look at convergence.


```{python}
learning_rate = 0.01
n_iter = 1000
grad = grad_OLS()

gd = GradientDescent(learning_rate, grad)

step_size = 10
n_steps = int(n_iter / step_size)
convergence = np.zeros((n_steps,X.shape[1]))
print(convergence.shape)
iters = np.zeros(n_steps)
for i in range(n_steps):
    gd.descend(X, y, epochs = step_size)
    new_coef = gd.theta
    convergence[i, :] = (poly_list.reshape(-1, 1) - new_coef).flatten()
    iters[i] = i*step_size

print(gd.theta)

for i in range(X.shape[1]):
    plt.plot(iters, convergence[:,i], label=fr"$\beta_{i}$")

plt.legend()
plt.axhline(0, linestyle = "--", color="grey")
plt.show()
```

The values converge at the true values over time! With (quite a bit of) noise they converge at slightly different values:

```{python}
# add noise
yn = y + np.random.normal(0, 1, (n,1))

learning_rate = 0.01
n_iter = 1000
grad = grad_OLS()

gd = GradientDescent(learning_rate, grad)

step_size = 10
n_steps = int(n_iter / step_size)
convergence = np.zeros((n_steps,X.shape[1]))
print(convergence.shape)
iters = np.zeros(n_steps)
for i in range(n_steps):
    gd.descend(X, yn, epochs = step_size)
    new_coef = gd.theta
    convergence[i, :] = (poly_list.reshape(-1, 1) - new_coef).flatten()
    iters[i] = i*step_size

print(gd.theta)

for i in range(X.shape[1]):
    plt.plot(iters, convergence[:,i], label=fr"$\beta_{i}$")

plt.legend()
plt.axhline(0, linestyle = "--", color="grey")
plt.show()
```

We stay with the no-noise data, and check how momentum affect convergence times.


```{python}
learning_rate = 0.01
n_iter = 1000
grad = grad_OLS()

gd = GradientDescent(learning_rate, grad, optimizer=Momentum(0.3))

step_size = 10
n_steps = int(n_iter / step_size)
convergence = np.zeros((n_steps,X.shape[1]))
print(convergence.shape)
iters = np.zeros(n_steps)
for i in range(n_steps):
    gd.descend(X, y, epochs = step_size)
    new_coef = gd.theta
    convergence[i, :] = (poly_list.reshape(-1, 1) - new_coef).flatten()
    iters[i] = i*step_size

print(gd.theta)

for i in range(X.shape[1]):
    plt.plot(iters, convergence[:,i], label=fr"$\beta_{i}$")

plt.legend()
plt.axhline(0, linestyle = "--", color="grey")
plt.show()
```

The parameters converge quicker with momentum. Try another optimizer:


```{python}
learning_rate = 0.01
n_iter = 1000
grad = grad_OLS()

gd = GradientDescent(learning_rate, grad, optimizer = ADAM())

step_size = 10
n_steps = int(n_iter / step_size)
convergence = np.zeros((n_steps,X.shape[1]))
print(convergence.shape)
iters = np.zeros(n_steps)
for i in range(n_steps):
    gd.descend(X, y, epochs = step_size)
    new_coef = gd.theta
    convergence[i, :] = (poly_list.reshape(-1, 1) - new_coef).flatten()
    iters[i] = i*step_size

print(gd.theta)

for i in range(X.shape[1]):
    plt.plot(iters, convergence[:,i], label=fr"$\beta_{i}$")

plt.legend()
plt.axhline(0, linestyle = "--", color="grey")
plt.show()
```

It looks like it's heading there slowly, but ADAM performs poorly for this particular data and learning rate. Try another learning rate:


```{python}
learning_rate = 1
n_iter = 1000
grad = grad_OLS()

gd = GradientDescent(learning_rate, grad, optimizer = ADAM())

step_size = 10
n_steps = int(n_iter / step_size)
convergence = np.zeros((n_steps,X.shape[1]))
print(convergence.shape)
iters = np.zeros(n_steps)
for i in range(n_steps):
    gd.descend(X, y, epochs = step_size)
    new_coef = gd.theta
    convergence[i, :] = (poly_list.reshape(-1, 1) - new_coef).flatten()
    iters[i] = i*step_size

print(gd.theta)

for i in range(X.shape[1]):
    plt.plot(iters, convergence[:,i], label=fr"$\beta_{i}$")

plt.legend()
plt.axhline(0, linestyle = "--", color="grey")
plt.show()
```

ADAM is considerably faster than just plain momentum when changing the learning rate (which is a bit unfair, since we didn't tweak any momentum parameters ...).

Finally we do the same but for SGD, so we know that that works too (but it's not likely to get stuck in local minima for this simple function).


```{python}
learning_rate = 0.01
n_iter = 1000
grad = grad_OLS()

gd = GradientDescent(learning_rate, grad, optimizer = ADAM())

step_size = 10
n_steps = int(n_iter / step_size)
convergence = np.zeros((n_steps,X.shape[1]))
print(convergence.shape)
iters = np.zeros(n_steps)
for i in range(n_steps):
    gd.descend(X, y, epochs = step_size, batch_size = 5)
    new_coef = gd.theta
    convergence[i, :] = (poly_list.reshape(-1, 1) - new_coef).flatten()
    iters[i] = i*step_size

print(gd.theta)

for i in range(X.shape[1]):
    plt.plot(iters, convergence[:,i], label=fr"$\beta_{i}$")

plt.legend()
plt.axhline(0, linestyle = "--", color="grey")
plt.show()
```

This also converges really quickly (but keeps moving due to the stochasticity), but again we have tweaked the parameters a little bit. I will tweak more systematically when using more complex data.

# Franke function

We generate data using the Franke function, and polynomial features. We found polynomial degree 10 to be the best fit in project 1, so we use that as a starting point, and look at combinations of learning rates and the regularization parameter $\lambda$

```{python}
# Make data.
x1 = np.arange(0, 1, 0.05)
x2 = np.arange(0, 1, 0.05)
x1, x2 = np.meshgrid(x1,x2)

z = franke_function(x1, x2) #+ np.random.normal(0,0.1,x1.shape)

x1x2 = np.column_stack((x1.flatten(), x2.flatten()))
max_poly = 10
X_feat = PolynomialFeatures(max_poly).fit_transform(x1x2)
X = X_feat[:, 1:]
y = z.flatten()

np.random.seed(42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# reshape y to 2D array
y_test = y_test[:,np.newaxis]
y_train = y_train[:,np.newaxis]

# scale data
scalerX = StandardScaler(with_std=True)
X_train_scaled = scalerX.fit_transform(X_train)
X_test_scaled = scalerX.transform(X_test)

scalery = StandardScaler(with_std=True)
y_train_scaled = scalery.fit_transform(y_train)
y_test_scaled = scalery.transform(y_test)

```

We work with standardized data without intercept, and try to minimize MSE of the test data for different combinations of $\lambda$ and learning rates.

## Learning rates and optimizers

First we try changing learning rate with a $\lambda$ of $10^{-4}$, which we found to be the optimal value in project 1. Trying also with different optimizers.

```{python}
learning_rate=0.01
n_iter = 200
lmb=1e-4
grad = grad_ridge(lmb)
grad=grad_OLS()
gd = GradientDescent(learning_rate, grad)
gd.descend(X_train_scaled, y_train_scaled, n_iter)
print(gd.theta)

```

```{python}
np.random.seed(3489)
learning_rates = np.logspace(-8,-3, 12)
optimizers = [Momentum, AdaGrad, ADAM, RMSProp]
n_iter = 1000
lmb=1e-4
grad = grad_ridge(lmb)

mses = np.zeros( (len(learning_rates), len(optimizers)) )
for j, optimizer in enumerate(optimizers):
    for i, learning_rate in enumerate(learning_rates):
        gd = GradientDescent(learning_rate, grad, optimizer=optimizer())
        gd.descend(X_train_scaled, y_train_scaled, n_iter)
        y_pred = X_test_scaled @ gd.theta
        mses[i,j] = mse(y_pred, y_test_scaled)
```


```{python}
optimizer_names = ["Momentum", "AdaGrad", "ADAM", "RMSProp"]
for j in range(1,len(optimizers)):
    plt.plot(learning_rates, mses[:,j], label=optimizer_names[j])
plt.legend()
plt.xscale("log")
plt.yscale("log")
plt.show()
```

Seems a bit random how learning rate affects the mse, other than that the optimum seems to be quite low for this data.

## Learning rate and $\lambda$ grid search

Start with ADAM


```{python}
def GD_lambda_mse(
    X_train, X_test, y_train, y_test, learning_rate, lmbs, n_iter, batch_size=None, optimizer=None, gradient_fun=grad_ridge):
    mses=np.zeros(len(lmbs))
    for i in range(len(lmbs)):
        gd = GradientDescent(learning_rate, gradient_fun(lmbs[i]), optimizer=optimizer)
        gd.descend(X_train, y_train, n_iter, batch_size)
        y_pred = X_test @ gd.theta
        mses[i] = mse(y_test, y_pred)
    return mses

def eta_lambda_grid(
    X_train, X_test, y_train, y_test, learning_rates, lmbs, n_iter, batch_size=None, optimizer=None, gradient_fun=grad_ridge
    ):
    mses = np.zeros( (len(lmbs), len(learning_rates)) )
    for i in range(len(learning_rates)):
        mse_eta = GD_lambda_mse(
            X_train, X_test, y_train, y_test, learning_rate=learning_rates[i], lmbs=lmbs, n_iter=n_iter, batch_size=batch_size, optimizer=optimizer, gradient_fun=gradient_fun
            )
        mses[:,i] = mse_eta
    return mses

def lambda_lr_heatmap(mses, lmbs, learning_rates, lmb_label_res=3, lr_label_res=3):
    lmb_lab = ["{0:.2e}".format(x) for x in lmbs]
    lr_lab = ["{0:.2e}".format(x) for x in learning_rates]

    sns.heatmap(mses, annot=True)
    plt.xticks(np.arange(len(learning_rates))[1::lmb_label_res] + 0.5, lr_lab[1::lmb_label_res])
    plt.yticks(np.arange(len(lmbs))[1::lr_label_res] + 0.5, lmb_lab[1::lr_label_res])
    plt.xlabel(r"Learning rate $\eta$")
    plt.ylabel(r"$\lambda$")
    plt.show()
```


```{python}
np.random.seed(8654)
learning_rates = np.logspace(-8,-5, 8)
n_iter = 200
lmbs=np.logspace(-8,1, 8)

mses = eta_lambda_grid(
    X_train, X_test, y_train, y_test,
    learning_rates, lmbs, n_iter, optimizer=ADAM(), batch_size = 5
)
plt.title("ADAM")
lambda_lr_heatmap(mses, lmbs, learning_rates)
```

```{python}
np.random.seed(34)
learning_rates = np.logspace(-8,-5, 8)
n_iter = 200
lmbs=np.logspace(-8,1, 8)

mses = eta_lambda_grid(
    X_train, X_test, y_train, y_test,
    learning_rates, lmbs, n_iter, optimizer=AdaGrad(), batch_size = 5
)

plt.title("AdaGrad")
lambda_lr_heatmap(mses, lmbs, learning_rates)
```

```{python}
np.random.seed(34)
learning_rates = np.logspace(-8,-5, 8)
n_iter = 200
lmbs=np.logspace(-8,1, 8)

mses = eta_lambda_grid(
    X_train, X_test, y_train, y_test,
    learning_rates, lmbs, n_iter, optimizer=RMSProp(), batch_size = 5
)

plt.title("RMSProp")
lambda_lr_heatmap(mses, lmbs, learning_rates)
```

```{python}
np.random.seed(34)
learning_rates = np.logspace(-8,-3, 8)
n_iter = 200
lmbs=np.logspace(-8,1, 8)

mses = eta_lambda_grid(
    X_train, X_test, y_train, y_test,
    learning_rates, lmbs, n_iter, optimizer=Momentum(), batch_size = 5
)

plt.title("Momentum")
lambda_lr_heatmap(mses, lmbs, learning_rates)
```
